{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.14.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Using cached anyio-4.3.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.6.4-py3-none-any.whl.metadata (85 kB)\n",
      "     ---------------------------------------- 0.0/85.1 kB ? eta -:--:--\n",
      "     -------------------------------------- - 81.9/85.1 kB 2.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 85.1/85.1 kB 1.6 MB/s eta 0:00:00\n",
      "Collecting sniffio (from openai)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\ayush singh\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from openai) (4.10.0)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai)\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting exceptiongroup>=1.0.2 (from anyio<5,>=3.5.0->openai)\n",
      "  Using cached exceptiongroup-1.2.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai)\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.4-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.16.3 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.16.3-cp38-none-win_amd64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\ayush singh\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading openai-1.14.2-py3-none-any.whl (262 kB)\n",
      "   ---------------------------------------- 0.0/262.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 262.4/262.4 kB 5.4 MB/s eta 0:00:00\n",
      "Using cached anyio-4.3.0-py3-none-any.whl (85 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "   ---------------------------------------- 0.0/75.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 75.6/75.6 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
      "   ---------------------------------------- 0.0/77.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 77.8/77.8 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.6.4-py3-none-any.whl (394 kB)\n",
      "   ---------------------------------------- 0.0/394.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 394.9/394.9 kB 8.2 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.16.3-cp38-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 0.8/1.9 MB 17.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.6/1.9 MB 16.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.9/1.9 MB 17.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 13.3 MB/s eta 0:00:00\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Using cached exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: tqdm, sniffio, pydantic-core, idna, h11, exceptiongroup, distro, certifi, annotated-types, pydantic, httpcore, anyio, httpx, openai\n",
      "Successfully installed annotated-types-0.6.0 anyio-4.3.0 certifi-2024.2.2 distro-1.9.0 exceptiongroup-1.2.0 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 idna-3.6 openai-1.14.2 pydantic-2.6.4 pydantic-core-2.16.3 sniffio-1.3.1 tqdm-4.66.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = my_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = openai.models.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = list(all_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(id='gpt-3.5-turbo-1106', created=1698959748, object='model', owned_by='system'),\n",
       " Model(id='dall-e-3', created=1698785189, object='model', owned_by='system'),\n",
       " Model(id='dall-e-2', created=1698798177, object='model', owned_by='system'),\n",
       " Model(id='whisper-1', created=1677532384, object='model', owned_by='openai-internal'),\n",
       " Model(id='tts-1-hd-1106', created=1699053533, object='model', owned_by='system'),\n",
       " Model(id='tts-1-hd', created=1699046015, object='model', owned_by='system'),\n",
       " Model(id='gpt-3.5-turbo', created=1677610602, object='model', owned_by='openai'),\n",
       " Model(id='gpt-3.5-turbo-0613', created=1686587434, object='model', owned_by='openai'),\n",
       " Model(id='gpt-3.5-turbo-0301', created=1677649963, object='model', owned_by='openai'),\n",
       " Model(id='gpt-3.5-turbo-16k-0613', created=1685474247, object='model', owned_by='openai'),\n",
       " Model(id='text-embedding-3-large', created=1705953180, object='model', owned_by='system'),\n",
       " Model(id='gpt-3.5-turbo-16k', created=1683758102, object='model', owned_by='openai-internal'),\n",
       " Model(id='gpt-3.5-turbo-instruct', created=1692901427, object='model', owned_by='system'),\n",
       " Model(id='gpt-3.5-turbo-instruct-0914', created=1694122472, object='model', owned_by='system'),\n",
       " Model(id='tts-1', created=1681940951, object='model', owned_by='openai-internal'),\n",
       " Model(id='davinci-002', created=1692634301, object='model', owned_by='system'),\n",
       " Model(id='babbage-002', created=1692634615, object='model', owned_by='system'),\n",
       " Model(id='tts-1-1106', created=1699053241, object='model', owned_by='system'),\n",
       " Model(id='gpt-3.5-turbo-0125', created=1706048358, object='model', owned_by='system'),\n",
       " Model(id='text-embedding-ada-002', created=1671217299, object='model', owned_by='openai-internal'),\n",
       " Model(id='text-embedding-3-small', created=1705948997, object='model', owned_by='system')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created</th>\n",
       "      <th>object</th>\n",
       "      <th>owned_by</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(id, gpt-3.5-turbo-1106)</td>\n",
       "      <td>(created, 1698959748)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(id, dall-e-3)</td>\n",
       "      <td>(created, 1698785189)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(id, dall-e-2)</td>\n",
       "      <td>(created, 1698798177)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(id, whisper-1)</td>\n",
       "      <td>(created, 1677532384)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, openai-internal)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(id, tts-1-hd-1106)</td>\n",
       "      <td>(created, 1699053533)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(id, tts-1-hd)</td>\n",
       "      <td>(created, 1699046015)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(id, gpt-3.5-turbo)</td>\n",
       "      <td>(created, 1677610602)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, openai)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(id, gpt-3.5-turbo-0613)</td>\n",
       "      <td>(created, 1686587434)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, openai)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(id, gpt-3.5-turbo-0301)</td>\n",
       "      <td>(created, 1677649963)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, openai)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(id, gpt-3.5-turbo-16k-0613)</td>\n",
       "      <td>(created, 1685474247)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, openai)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(id, text-embedding-3-large)</td>\n",
       "      <td>(created, 1705953180)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(id, gpt-3.5-turbo-16k)</td>\n",
       "      <td>(created, 1683758102)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, openai-internal)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(id, gpt-3.5-turbo-instruct)</td>\n",
       "      <td>(created, 1692901427)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(id, gpt-3.5-turbo-instruct-0914)</td>\n",
       "      <td>(created, 1694122472)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(id, tts-1)</td>\n",
       "      <td>(created, 1681940951)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, openai-internal)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(id, davinci-002)</td>\n",
       "      <td>(created, 1692634301)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(id, babbage-002)</td>\n",
       "      <td>(created, 1692634615)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(id, tts-1-1106)</td>\n",
       "      <td>(created, 1699053241)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(id, gpt-3.5-turbo-0125)</td>\n",
       "      <td>(created, 1706048358)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(id, text-embedding-ada-002)</td>\n",
       "      <td>(created, 1671217299)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, openai-internal)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(id, text-embedding-3-small)</td>\n",
       "      <td>(created, 1705948997)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   id                created           object  \\\n",
       "0            (id, gpt-3.5-turbo-1106)  (created, 1698959748)  (object, model)   \n",
       "1                      (id, dall-e-3)  (created, 1698785189)  (object, model)   \n",
       "2                      (id, dall-e-2)  (created, 1698798177)  (object, model)   \n",
       "3                     (id, whisper-1)  (created, 1677532384)  (object, model)   \n",
       "4                 (id, tts-1-hd-1106)  (created, 1699053533)  (object, model)   \n",
       "5                      (id, tts-1-hd)  (created, 1699046015)  (object, model)   \n",
       "6                 (id, gpt-3.5-turbo)  (created, 1677610602)  (object, model)   \n",
       "7            (id, gpt-3.5-turbo-0613)  (created, 1686587434)  (object, model)   \n",
       "8            (id, gpt-3.5-turbo-0301)  (created, 1677649963)  (object, model)   \n",
       "9        (id, gpt-3.5-turbo-16k-0613)  (created, 1685474247)  (object, model)   \n",
       "10       (id, text-embedding-3-large)  (created, 1705953180)  (object, model)   \n",
       "11            (id, gpt-3.5-turbo-16k)  (created, 1683758102)  (object, model)   \n",
       "12       (id, gpt-3.5-turbo-instruct)  (created, 1692901427)  (object, model)   \n",
       "13  (id, gpt-3.5-turbo-instruct-0914)  (created, 1694122472)  (object, model)   \n",
       "14                        (id, tts-1)  (created, 1681940951)  (object, model)   \n",
       "15                  (id, davinci-002)  (created, 1692634301)  (object, model)   \n",
       "16                  (id, babbage-002)  (created, 1692634615)  (object, model)   \n",
       "17                   (id, tts-1-1106)  (created, 1699053241)  (object, model)   \n",
       "18           (id, gpt-3.5-turbo-0125)  (created, 1706048358)  (object, model)   \n",
       "19       (id, text-embedding-ada-002)  (created, 1671217299)  (object, model)   \n",
       "20       (id, text-embedding-3-small)  (created, 1705948997)  (object, model)   \n",
       "\n",
       "                       owned_by  \n",
       "0            (owned_by, system)  \n",
       "1            (owned_by, system)  \n",
       "2            (owned_by, system)  \n",
       "3   (owned_by, openai-internal)  \n",
       "4            (owned_by, system)  \n",
       "5            (owned_by, system)  \n",
       "6            (owned_by, openai)  \n",
       "7            (owned_by, openai)  \n",
       "8            (owned_by, openai)  \n",
       "9            (owned_by, openai)  \n",
       "10           (owned_by, system)  \n",
       "11  (owned_by, openai-internal)  \n",
       "12           (owned_by, system)  \n",
       "13           (owned_by, system)  \n",
       "14  (owned_by, openai-internal)  \n",
       "15           (owned_by, system)  \n",
       "16           (owned_by, system)  \n",
       "17           (owned_by, system)  \n",
       "18           (owned_by, system)  \n",
       "19  (owned_by, openai-internal)  \n",
       "20           (owned_by, system)  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(models_list, columns= ['id','created','object','owned_by'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom openai import OpenAI\\nclient = OpenAI()\\n\\nresponse = client.chat.completions.create(\\n  model=\"gpt-3.5-turbo\",\\n  messages=[\\n    {\\n      \"role\": \"system\",\\n      \"content\": \"You are a helpful assistant\"\\n    },\\n    {\\n      \"role\": \"user\",\\n      \"content\": \"how I can make money\"\\n    },\\n    {\\n      \"role\": \"assistant\",\\n      \"content\": \"There are many ways to make money, depending on your skills, interests, and resources. Here are some suggestions:\\n\\n1. Get a job: Look for employment opportunities that match your skills and experience. You can search for job openings online, in newspapers, or through networking.\\n\\n2. Freelancing: Offer your services as a freelancer on platforms like Upwork, Fiverr, or Freelancer. This could include writing, graphic design, programming, and more.\\n\\n3. Start a business: If you have a business idea, consider starting your own company. You could sell products or services online or in person.\\n\\n4. Invest: Invest your money in assets like stocks, real estate, or businesses to potentially grow your wealth over time.\\n\\n5. Passive income: Explore ways to generate passive income, such as through rental properties, investments, or creating digital products.\\n\\n6. Side hustles: Take on side gigs or part-time work to earn extra money outside of your main job.\\n\\nRemember, it\\'s important to be mindful of scams and to always research any money-making opportunity before committing to it.\"\\n    }\\n  ],\\n  temperature=1,\\n  max_tokens=256,\\n  top_p=1,\\n  frequency_penalty=0,\\n  presence_penalty=0\\n)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"You are a helpful assistant\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"how I can make money\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"There are many ways to make money, depending on your skills, interests, and resources. Here are some suggestions:\\n\\n1. Get a job: Look for employment opportunities that match your skills and experience. You can search for job openings online, in newspapers, or through networking.\\n\\n2. Freelancing: Offer your services as a freelancer on platforms like Upwork, Fiverr, or Freelancer. This could include writing, graphic design, programming, and more.\\n\\n3. Start a business: If you have a business idea, consider starting your own company. You could sell products or services online or in person.\\n\\n4. Invest: Invest your money in assets like stocks, real estate, or businesses to potentially grow your wealth over time.\\n\\n5. Passive income: Explore ways to generate passive income, such as through rental properties, investments, or creating digital products.\\n\\n6. Side hustles: Take on side gigs or part-time work to earn extra money outside of your main job.\\n\\nRemember, it's important to be mindful of scams and to always research any money-making opportunity before committing to it.\"\n",
    "    }\n",
    "  ],\n",
    "  temperature=1,\n",
    "  max_tokens=256,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0,\n",
    "  presence_penalty=0\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=my_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"You are a helpful assistant\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"how I can make money\"\n",
    "    },\n",
    "    \n",
    "  ],\n",
    "  temperature=1,\n",
    "  max_tokens=20,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0,\n",
    "  presence_penalty=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-961o5tXgewy66I8QYGAj87wefGr0N', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='There are many ways you can make money, depending on your skill set, interests, and available resources', role='assistant', function_call=None, tool_calls=None))], created=1711223941, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_3bc1b5746c', usage=CompletionUsage(completion_tokens=20, prompt_tokens=21, total_tokens=41))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "openai.types.chat.chat_completion.ChatCompletion"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='There are many ways you can make money, depending on your skill set, interests, and available resources', role='assistant', function_call=None, tool_calls=None))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are many ways you can make money, depending on your skill set, interests, and available resources'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Day 2 Lang Chanin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_description = \"Ayush Singh is student of the Electrical and Electronics Engineering at Galgotias University in 2018 , He graduated with 7.87 CGPA the Persue a Data Science Cource fron iNeuron.ai and currently working as SOA and BI Developer and looking for a change in Data Science and Gen AI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ayush Singh is student of the Electrical and Electronics Engineering at Galgotias University in 2018 , He graduated with 7.87 CGPA the Persue a Data Science Cource fron iNeuron.ai and currently working as SOA and BI Developer and looking for a change in Data Science and Gen AI'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A siple prompt to extract information from \"student_description\" in json format\n",
    "prompt = f\"\"\"\n",
    "Please extract the following information from given text and return it as json object:\n",
    "\n",
    "name\n",
    "collage\n",
    "grades\n",
    "subject\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_description}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPlease extract the following information from given text and return it as json object:\\n\\nname\\ncollage\\ngrades\\nsubject\\n\\nThis is the body of text to extract the information from:\\nAyush Singh is student of the Electrical and Electronics Engineering at Galgotias University in 2018 , He graduated with 7.87 CGPA the Persue a Data Science Cource fron iNeuron.ai and currently working as SOA and BI Developer and looking for a change in Data Science and Gen AI\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<openai.OpenAI at 0x280b7718940>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=my_key)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": prompt\n",
    "    },\n",
    "    \n",
    "  ]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-96E0zvSBGok6exp8vO9AaHEU6L4Zm', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n  \"name\": \"Ayush Singh\",\\n  \"college\": \"Galgotias University\",\\n  \"grades\": 7.87,\\n  \"subject\": \"Electrical and Electronics Engineering\"\\n}', role='assistant', function_call=None, tool_calls=None))], created=1711270869, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_3bc1b5746c', usage=CompletionUsage(completion_tokens=41, prompt_tokens=108, total_tokens=149))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Ayush Singh',\n",
       " 'college': 'Galgotias University',\n",
       " 'grades': 7.87,\n",
       " 'subject': 'Electrical and Electronics Engineering'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json.loads(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_custom_function = [\n",
    "    {\n",
    "        'name': 'extract_student_info',\n",
    "        'description': 'Get student information from the body of the input text',\n",
    "        'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "                'name': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'Name of the student'\n",
    "                },\n",
    "                'collage': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'The collage name'\n",
    "                },\n",
    "                'grades': {\n",
    "                    'type': 'integer',\n",
    "                    'description': 'CGPA of the student'\n",
    "                },\n",
    "                'subjects': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'subject of the student which he studied in the collage'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[{\"role\": \"user\",\"content\": prompt}],\n",
    "  functions = student_custom_function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-96EuqfKTpJNWU3tbjFSt5wgpkFx01', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"name\":\"Ayush Singh\",\"collage\":\"Galgotias University\",\"grades\":7.87,\"subjects\":\"Electrical and Electronics Engineering\"}', name='extract_student_info'), tool_calls=None))], created=1711274332, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_3bc1b5746c', usage=CompletionUsage(completion_tokens=39, prompt_tokens=195, total_tokens=234))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"name\":\"Ayush Singh\",\"collage\":\"Galgotias University\",\"grades\":7.87,\"subjects\":\"Electrical and Electronics Engineering\"}'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2.choices[0].message.function_call.arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Ayush Singh',\n",
       " 'collage': 'Galgotias University',\n",
       " 'grades': 7.87,\n",
       " 'subjects': 'Electrical and Electronics Engineering'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(response2.choices[0].message.function_call.arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ayush Singh is student of the Electrical and Electronics Engineering at Galgotias University in 2018 , He graduated with 7.87 CGPA the Persue a Data Science Cource fron iNeuron.ai and currently working as SOA and BI Developer and looking for a change in Data Science and Gen AI'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_description2 = 'Kartikaye is student of the Electrical Engineering at Amity University in 2018 , He graduated with 8.5 CGPA then Persued a Data Science Cource fron iNeuron.ai and currently working as SOA and BI Developer and looking for a change in Data Science and Gen AI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Ayush Singh', 'collage': 'Galgotias University', 'grades': 7.87, 'subjects': 'Electrical and Electronics Engineering'}\n",
      "{'name': 'Kartikaye', 'collage': 'Amity University', 'grades': 8.5, 'subjects': 'Electrical Engineering'}\n"
     ]
    }
   ],
   "source": [
    "student_info = [student_description, student_description2]\n",
    "for student in student_info:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\",\"content\": student}],\n",
    "        functions = student_custom_function,\n",
    "        function_call = 'auto'\n",
    "    )\n",
    "\n",
    "    response = json.loads(response.choices[0].message.function_call.arguments)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Advance Example of Function Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model= \"gpt-3.5-turbo\",\n",
    "    messages= [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': \"When's the next flight from delhi to mumbai?\"\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am an AI assistant and do not have real-time information on flight schedules. I recommend checking the'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_description = [\n",
    "    {\n",
    "        \"name\":  \"get_flight_info\",\n",
    "        \"description\": \"Get flight information between two locations\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"loc_origin\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The departure airport e.g. DEL\",\n",
    "                },\n",
    "                \"loc_destination\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The destination airport e.g. MUM\",\n",
    "                },\n",
    "            },\n",
    "            \"required\" : [\"loc_origin\", \"loc_destination\"],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"When's the next flight from Delhi to Mumbai?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "response3 = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[{\"role\": \"user\",\"content\": user_prompt}],\n",
    "  functions = function_description\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"loc_origin\":\"DEL\",\"loc_destination\":\"BOM\"}'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response3.choices[0].message.function_call.arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check rapid api and call MMT api for flight booking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "def get_flight_info(loc_origin, loc_destination):\n",
    "    \"\"\" Get a flight booking between two locations\"\"\"\n",
    "    # Example API return from DataBase \n",
    "\n",
    "    flight_info = {\n",
    "        \"loc_origin\" : loc_origin,\n",
    "        \"locals_destination\" : loc_destination,\n",
    "        \"datetime\" : str(datetime.now() + timedelta(hours = 2)),\n",
    "        \"airline\": \"KLM\",\n",
    "        \"flight\": \"KL643\",\n",
    "    }\n",
    "    return json.dumps(flight_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = json.loads(response3.choices[0].message.function_call.arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DEL'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(response3.choices[0].message.function_call.arguments)['loc_origin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BOM'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(response3.choices[0].message.function_call.arguments).get('loc_destination')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = json.loads(response3.choices[0].message.function_call.arguments).get('loc_origin')\n",
    "destination = json.loads(response3.choices[0].message.function_call.arguments).get('loc_destination')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'get_flight_info'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response3.choices[0].message.function_call.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response3.choices[0].message.function_call.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.get_flight_info(loc_origin, loc_destination)>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(response3.choices[0].message.function_call.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.get_flight_info(loc_origin, loc_destination)>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_function = eval(response3.choices[0].message.function_call.name)\n",
    "chosen_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loc_origin': 'DEL', 'loc_destination': 'BOM'}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loc_origin\": \"DEL\", \"locals_destination\": \"BOM\", \"datetime\": \"2024-03-24 19:20:00.553280\", \"airline\": \"KLM\", \"flight\": \"KL643\"}\n"
     ]
    }
   ],
   "source": [
    "flight = chosen_function(**params)\n",
    "print(flight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"When's the next flight from Delhi to Mumbai?\""
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "response4 = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[{\"role\": \"user\",\"content\": user_prompt},\n",
    "            {\"role\": \"function\",\"name\": response3.choices[0].message.function_call.name, \"content\": flight}\n",
    "             ],\n",
    "  #Add function calling\n",
    "  functions = function_description,\n",
    "  function_call = \"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-96Go5mDFI4MZebgFLYoh4dqDjRhND', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The next flight from Delhi (DEL) to Mumbai (BOM) is on March 24, 2024 at 19:20 with KLM Airlines (Flight: KL643).', role='assistant', function_call=None, tool_calls=None))], created=1711281601, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_3bc1b5746c', usage=CompletionUsage(completion_tokens=39, prompt_tokens=138, total_tokens=177))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The next flight from Delhi (DEL) to Mumbai (BOM) is on March 24, 2024 at 19:20 with KLM Airlines (Flight: KL643).'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response4.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ayush Singh\\anaconda3\\envs\\testingopenai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "my_key = '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'\n",
    "client = OpenAI(openai_api_key = my_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero shot prompting\n",
    "prompt = \"can you tell me total number of country in asia? can you give me top 10 country names?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a language model AI, I can provide you with accurate information. As of 2021, there are 48 countries in Asia. Here are the top 10 countries in Asia by population:\n",
      "\n",
      "1. China\n",
      "2. India\n",
      "3. Indonesia\n",
      "4. Pakistan\n",
      "5. Bangladesh\n",
      "6. Japan\n",
      "7. Philippines\n",
      "8. Vietnam\n",
      "9. Iran\n",
      "10. Turkey\n"
     ]
    }
   ],
   "source": [
    "print(client.predict(prompt).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = \"can you tell me capital of India?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of India is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "print(client.predict(prompt2).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Prompt Templates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables=['country'],\n",
    "    template = \"can you tell me the capital of {country}?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = prompt_template_name.format(country = \"Delhi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = prompt_template_name.format(country = \"China\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Delhi is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "print(client.predict(prompt1).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of China is Beijing.\n"
     ]
    }
   ],
   "source": [
    "print(client.predict(prompt2).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"what is good name of company that make a {product}?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is good name of company that make a toys?'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(product = \"toys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt3 = prompt.format(product = \"toys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some potential good names for a toy company could be:\n",
      "\n",
      "1. Playful Creations\n",
      "2. Imagination Inc.\n",
      "3. Wonder World Toys\n",
      "4. Dreamland Designs\n",
      "5. Magical Playthings\n",
      "6. Fun Factory Toys\n",
      "7. Little Wonders Co.\n",
      "8. Toytopia\n",
      "9. Happy Kid Co.\n",
      "10. Whimsical Toys Co.\n"
     ]
    }
   ],
   "source": [
    "print(client.predict(prompt3).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "serpapi_key = \"ad5e4311ac7f1abf721f0a59777c9090aa4e24f5b0bfb1bc63b698ade9ccd81f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(openai_api_key = my_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = load_tools(['serpapi'], serpapi_api_key = serpapi_key, llm  = client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(tool, client, agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should search for the latest news on the cricket worldcup.\n",
      "Action: Search\n",
      "Action Input: \"cricket worldcup winner\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAustralian Mens Cricket Team\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should search for more specific information on the Australian Mens Cricket Team.\n",
      "Action: Search\n",
      "Action Input: \"Australian Mens Cricket Team 2021\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{'title': 'Australian Mens Cricket Team', 'thumbnail': 'https://serpapi.com/searches/6600855c11c88118232b4ab5/images/857392c4257778cd5d5e40f7a505186c085d64546f551ce6b5f8c90edb87fd0b.png', 'games': [{'tournament': 'International Cricket Council', 'stadium': 'MCG', 'date': 'Dec 2530, 2021', 'teams': [{'name': 'England', 'score': '185 & 68', 'thumbnail': 'https://serpapi.com/searches/6600855c11c88118232b4ab5/images/857392c4257778cdc2d29ceb018d0bd2947d01b05754f15500f723909dd9b408251b20fd82bfd75b55a0ad81c9310bb3.png'}, {'name': 'Australia', 'score': '267', 'thumbnail': 'https://serpapi.com/searches/6600855c11c88118232b4ab5/images/857392c4257778cdc2d29ceb018d0bd2947d01b05754f15535bf57ad54f991519541384084b127466db487f0ddebcfd6.png'}], 'status': 'AUS won by an innings and 14 runs'}, {'tournament': 'International Cricket Council', 'stadium': 'Adelaide Oval', 'date': 'Dec 1520, 2021', 'teams': [{'name': 'Australia', 'score': '473/9d & 230/9d', 'thumbnail': 'https://serpapi.com/searches/6600855c11c88118232b4ab5/images/857392c4257778cdc2d29ceb018d0bd2b94ded9079aeb7f311af23e0d988e9ebcea94b22f6042e3b5e533b480f251eab.png'}, {'name': 'England', 'score': '236 & 192Target 468', 'thumbnail': 'https://serpapi.com/searches/6600855c11c88118232b4ab5/images/857392c4257778cdc2d29ceb018d0bd2b94ded9079aeb7f3d2dba435c5bd91dc1652b9138d6e7d98b3308b27f8eeda86.png'}], 'status': 'AUS won by 275 runs'}, {'tournament': 'International Cricket Council', 'stadium': 'The Gabba', 'date': 'Dec 712, 2021', 'teams': [{'name': 'England', 'score': '147 & 297', 'thumbnail': 'https://serpapi.com/searches/6600855c11c88118232b4ab5/images/857392c4257778cdc2d29ceb018d0bd20f2172865e7496a979b96718d5d6170f905a80c675a6d5042239301f98fb594c.png'}, {'name': 'Australia', 'score': '425 & 20/1Target 20', 'thumbnail': 'https://serpapi.com/searches/6600855c11c88118232b4ab5/images/857392c4257778cdc2d29ceb018d0bd20f2172865e7496a94f489356166e796ff01711a4a4491f07cd829dc8d21149c8.png'}], 'status': 'AUS won by 9 wickets'}, {'tournament': \"ICC Men's T20 World Cup\", 'stadium': 'Dubai International Stadium', 'stage': 'Final  T20 45 of 45', 'date': 'Nov 14, 21', 'teams': [{'name': 'New Zealand', 'score': '172/4 (20)', 'thumbnail': 'https://serpapi.com/searches/6600855c11c88118232b4ab5/images/857392c4257778cdc2d29ceb018d0bd27a835e0abbc7ae68d0485365281bd4d75a1864e3b897ce25ac0568f1ed470581.png'}, {'name': 'Australia', 'score': '173/2 (18.5)', 'thumbnail': 'https://serpapi.com/searches/6600855c11c88118232b4ab5/images/857392c4257778cdc2d29ceb018d0bd27a835e0abbc7ae685976a386af3bf5847be6c895573503184506d0f304e9770a.png'}], 'status': 'AUS won by 8 wickets (7 balls left)'}, {'tournament': \"ICC Men's T20 World Cup\", 'stadium': 'Dubai International Stadium', 'stage': 'Semi-final  T20 44 of 45', 'date': 'Nov 11, 21', 'teams': [{'name': 'Pakistan', 'score': '176/4 (20)', 'thumbnail': 'https://serpapi.com/searches/6600855c11c88118232b4ab5/images/857392c4257778cdc2d29ceb018d0bd2940e4eec717c7c40112b2dad6f27b7efb20ef259531163e16bf04b090aa29f1c.png'}, {'name': 'Australia', 'score': '177/5 (19)', 'thumbnail': 'https://serpapi.com/searches/6600855c11c88118232b4ab5/images/857392c4257778cdc2d29ceb018d0bd2940e4eec717c7c40069b24d020797c207bf6b62bac198ed82d8cc1dc5f663d49.png'}], 'status': 'AUS won by 5 wickets (6 balls left)'}, {'tournament': \"ICC Men's T20 World Cup\", 'stadium': 'Sheikh Zayed Stadium', 'stage': 'Super 12  T20 38 of 45', 'date': 'Nov 6, 21', 'teams': [{'name': 'West Indies', 'score': '157/7 (20)', 'thumbnail': 'https://serpapi.com/searches/6600855c11c88118232b4ab5/images/857392c4257778cdc2d29ceb018d0bd269280e504ed0dcf3b1797d51936f41b5c3c7decba5f034dcaf0a47a7902e0873.png'}, {'name': 'Australia', 'score': '161/2 (16.2)', 'thumbnail': 'https://serpapi.com/searches/6600855c11c88118232b4ab5/images/857392c4257778cdc2d29ceb018d0bd269280e504ed0dcf30906900e4936067c74e5f6893b699973e6505d3e69d3a9ff.png'}], 'status': 'AUS won by 8 wickets (22 balls left)'}]}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Based on the search results, it seems like Australia won the cricket worldcup in 2021.\n",
      "Final Answer: Australia\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Australia'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"can you tell me who won the cricket worldcup recently?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should think about how to find current affairs\n",
      "Action: Search\n",
      "Action Input: \"5 current affairs\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['A Current Affairs subscription is one of the best known ways to improve your life in a hurry. Our print magazine is released six times a year, in a beautiful ...', \"Current Affairs  Kotamreddy Demands Vijayasai Reddy's Rajya Sabha Resignation Over...  Kadapa Authorities Solve 193 of 194 MCC Cases  Gadchiroli Encounter Sends ...\", 'Here we are providing the daily Current Affairs 2024 for upcoming Bank, SSC, Railway and UPSC Exams. It will improve the current affairs and General ...', 'Check out for the latest news on current affairs along with current affairs live news at Times of India.', 'Latest world news headlines: International breaking news and current affairs from US, Europe, Middle East, Asia, Africa and more. If it happens around the ...', 'Daily Current Affairs Quiz: March 23, 2024. March 23, 2024. 1. What is the name of the operation recently launched by India to evacuate Indian Nationals ...', 'Current Affairs is an American bimonthly magazine that discusses political and cultural topics from a left-wing perspective. It was founded by Nathan J.', 'Feb 9, 2024 - Explore PendulumEdu\\'s board \"Current Affairs\", followed by 3581 people on Pinterest. See more ideas about current, affair, gernal knowledge.', 'Current Affairs 2024: Get here daily current affairs for UPSC IAS, Bank, SSC, Railway and all others competitive exams at Jagran Josh.', 'Get daily updates on latest Current Affairs 2024 for upcoming Bank, SSC, Railway, Defence, UPSC, UPPSC, BPSC, MPPSC, RPSC, KPSC, & all other Govt ...']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should keep searching for more information to have a well-rounded understanding of current affairs\n",
      "Action: Search\n",
      "Action Input: \"current affairs 2024\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[{'title': 'UPSC Weekly Current Affairs Quiz | March 17 to March 23, 2024', 'link': 'https://indianexpress.com/article/upsc-current-affairs/upsc-essentials/upsc-weekly-current-affairs-quiz-march-17-to-march-23-2024-9230396/', 'source': 'The Indian Express', 'date': '1 day ago', 'thumbnail': 'https://serpapi.com/searches/660085da272aa5e9e8da734c/images/04460b4f0c618a5cfa58cb38d88b37970e1582665d610eb4.jpeg'}, {'title': 'Current Affairs Questions and Answers 24th March 2024', 'link': 'https://www.bankersadda.com/current-affairs-questions-and-answers-24th-march-2024/', 'source': 'BankersAdda', 'date': '13 hours ago', 'thumbnail': 'https://serpapi.com/searches/660085da272aa5e9e8da734c/images/04460b4f0c618a5cc9abce1933bca46aaa26031b011a6225.jpeg'}, {'title': '[Mission 2024] INSIGHTS DAILY CURRENT AFFAIRS + PIB SUMMARY 23 March 2024', 'link': 'https://www.insightsonindia.com/2024/03/23/mission-2024-insights-daily-current-affairs-pib-summary-23-march-2024/', 'source': 'INSIGHTS IAS', 'date': '1 day ago'}]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should also check reliable news sources to ensure accuracy\n",
      "Action: Search\n",
      "Action Input: \"current affairs news sources\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['Breaking news and analysis from TIME.com. Politics, world news, photos, video, tech reviews, health, science and entertainment news.', 'A Current Affairs subscription is one of the best known ways to improve your life in a hurry. Our print magazine is released six times a year, in a beautiful ...', 'View the latest news and breaking news today for U.S., world, weather, entertainment, politics and health at CNN.com.', 'BBC, Reuters, Economist for general global affairs. Then you gotta look at regional and country specific news orgs.', 'I usually look at several news media from the US, England, Germany, Russia, China, Hong Kong, Japan, India and Africa to include Al Jazeera and ...', 'Quick Reference Sources  Google News. Comprehensive up-to-date news coverage, aggregated from sources all over the world by Google News.', \"The latest news, analysis, and data from the country each week ... The island's resistance to a dependableand desperately neededsource of energy has been shaped ...\", 'Read the latest headlines, breaking news, and videos at APNews.com, the definitive source for independent journalism from every corner of the globe.', 'Foreign Affairs is the leading magazine for in-depth analysis and debate of foreign policy, geopolitics and international affairs.', 'Get in-depth global news and analysis. Our coverage spans world politics, business, tech, culture and more. Subscribe for free trial.']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I have gathered enough information and am confident in my understanding of current affairs\n",
      "Final Answer: Some of the current affairs in 2024 include political and cultural topics, international news, and daily quizzes to test knowledge. Reliable news sources can include major media outlets and foreign affairs magazines.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Some of the current affairs in 2024 include political and cultural topics, international news, and daily quizzes to test knowledge. Reliable news sources can include major media outlets and foreign affairs magazines.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"can you tell me 5 current affairs?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = load_tools(['wikipedia'], llm = client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(tool, client, agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should use wikipedia to find information about the recent Cricket World Cup.\n",
      "Action: wikipedia\n",
      "Action Input: \"2019 Cricket World Cup\"\u001b[0m"
     ]
    },
    {
     "ename": "WikipediaException",
     "evalue": "An unknown error occured: \"Search is currently too busy. Please try again later.\". Please report it on GitHub!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWikipediaException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcan you tell me about this recent cricket worldcup in 250 words?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ayush Singh\\anaconda3\\envs\\testingopenai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     emit_warning()\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ayush Singh\\anaconda3\\envs\\testingopenai\\lib\\site-packages\\langchain\\chains\\base.py:545\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    544\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[0;32m    546\u001b[0m         _output_key\n\u001b[0;32m    547\u001b[0m     ]\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[0;32m    551\u001b[0m         _output_key\n\u001b[0;32m    552\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\Ayush Singh\\anaconda3\\envs\\testingopenai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     emit_warning()\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ayush Singh\\anaconda3\\envs\\testingopenai\\lib\\site-packages\\langchain\\chains\\base.py:378\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    347\u001b[0m \n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    371\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    376\u001b[0m }\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ayush Singh\\anaconda3\\envs\\testingopenai\\lib\\site-packages\\langchain\\chains\\base.py:163\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    162\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    164\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32mc:\\Users\\Ayush Singh\\anaconda3\\envs\\testingopenai\\lib\\site-packages\\langchain\\chains\\base.py:153\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    152\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 153\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    156\u001b[0m     )\n\u001b[0;32m    158\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    159\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    160\u001b[0m     )\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Ayush Singh\\anaconda3\\envs\\testingopenai\\lib\\site-packages\\langchain\\agents\\agent.py:1432\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m   1430\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[0;32m   1431\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m-> 1432\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1435\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1438\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[0;32m   1440\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[0;32m   1441\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[0;32m   1442\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Ayush Singh\\anaconda3\\envs\\testingopenai\\lib\\site-packages\\langchain\\agents\\agent.py:1138\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1131\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1136\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[1;32m-> 1138\u001b[0m         [\n\u001b[0;32m   1139\u001b[0m             a\n\u001b[0;32m   1140\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[0;32m   1141\u001b[0m                 name_to_tool_map,\n\u001b[0;32m   1142\u001b[0m                 color_mapping,\n\u001b[0;32m   1143\u001b[0m                 inputs,\n\u001b[0;32m   1144\u001b[0m                 intermediate_steps,\n\u001b[0;32m   1145\u001b[0m                 run_manager,\n\u001b[0;32m   1146\u001b[0m             )\n\u001b[0;32m   1147\u001b[0m         ]\n\u001b[0;32m   1148\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Ayush Singh\\anaconda3\\envs\\testingopenai\\lib\\site-packages\\langchain\\agents\\agent.py:1138\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1131\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1136\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[1;32m-> 1138\u001b[0m         [\n\u001b[0;32m   1139\u001b[0m             a\n\u001b[0;32m   1140\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[0;32m   1141\u001b[0m                 name_to_tool_map,\n\u001b[0;32m   1142\u001b[0m                 color_mapping,\n\u001b[0;32m   1143\u001b[0m                 inputs,\n\u001b[0;32m   1144\u001b[0m                 intermediate_steps,\n\u001b[0;32m   1145\u001b[0m                 run_manager,\n\u001b[0;32m   1146\u001b[0m             )\n\u001b[0;32m   1147\u001b[0m         ]\n\u001b[0;32m   1148\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Ayush Singh\\anaconda3\\envs\\testingopenai\\lib\\site-packages\\langchain\\agents\\agent.py:1223\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1221\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m agent_action\n\u001b[0;32m   1222\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent_action \u001b[38;5;129;01min\u001b[39;00m actions:\n\u001b[1;32m-> 1223\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_perform_agent_action\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ayush Singh\\anaconda3\\envs\\testingopenai\\lib\\site-packages\\langchain\\agents\\agent.py:1245\u001b[0m, in \u001b[0;36mAgentExecutor._perform_agent_action\u001b[1;34m(self, name_to_tool_map, color_mapping, agent_action, run_manager)\u001b[0m\n\u001b[0;32m   1243\u001b[0m         tool_run_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_prefix\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1244\u001b[0m     \u001b[38;5;66;03m# We then call the tool on the tool input to get an observation\u001b[39;00m\n\u001b[1;32m-> 1245\u001b[0m     observation \u001b[38;5;241m=\u001b[39m \u001b[43mtool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43magent_action\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_run_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1253\u001b[0m     tool_run_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mtool_run_logging_kwargs()\n",
      "File \u001b[1;32mc:\\Users\\Ayush Singh\\anaconda3\\envs\\testingopenai\\lib\\site-packages\\langchain_core\\tools.py:422\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[1;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    421\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_error(e)\n\u001b[1;32m--> 422\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    424\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_end(observation, color\u001b[38;5;241m=\u001b[39mcolor, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Ayush Singh\\anaconda3\\envs\\testingopenai\\lib\\site-packages\\langchain_core\\tools.py:381\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[1;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    378\u001b[0m     parsed_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_input(tool_input)\n\u001b[0;32m    379\u001b[0m     tool_args, tool_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_args_and_kwargs(parsed_input)\n\u001b[0;32m    380\u001b[0m     observation \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 381\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    383\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(\u001b[38;5;241m*\u001b[39mtool_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtool_kwargs)\n\u001b[0;32m    384\u001b[0m     )\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_validation_error:\n",
      "File \u001b[1;32mc:\\Users\\Ayush Singh\\anaconda3\\envs\\testingopenai\\lib\\site-packages\\langchain_community\\tools\\wikipedia\\tool.py:29\u001b[0m, in \u001b[0;36mWikipediaQueryRun._run\u001b[1;34m(self, query, run_manager)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run\u001b[39m(\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     25\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m     26\u001b[0m     run_manager: Optional[CallbackManagerForToolRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     27\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     28\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Use the Wikipedia tool.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_wrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ayush Singh\\anaconda3\\envs\\testingopenai\\lib\\site-packages\\langchain_community\\utilities\\wikipedia.py:46\u001b[0m, in \u001b[0;36mWikipediaAPIWrapper.run\u001b[1;34m(self, query)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     45\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run Wikipedia search and get page summaries.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m     page_titles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwiki_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mWIKIPEDIA_MAX_QUERY_LENGTH\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop_k_results\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     summaries \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m page_title \u001b[38;5;129;01min\u001b[39;00m page_titles[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k_results]:\n",
      "File \u001b[1;32mc:\\Users\\Ayush Singh\\anaconda3\\envs\\testingopenai\\lib\\site-packages\\wikipedia\\util.py:28\u001b[0m, in \u001b[0;36mcache.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache[key]\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 28\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\Ayush Singh\\anaconda3\\envs\\testingopenai\\lib\\site-packages\\wikipedia\\wikipedia.py:109\u001b[0m, in \u001b[0;36msearch\u001b[1;34m(query, results, suggestion)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPTimeoutError(query)\n\u001b[0;32m    108\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 109\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WikipediaException(raw_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    111\u001b[0m search_results \u001b[38;5;241m=\u001b[39m (d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m raw_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m suggestion:\n",
      "\u001b[1;31mWikipediaException\u001b[0m: An unknown error occured: \"Search is currently too busy. Please try again later.\". Please report it on GitHub!"
     ]
    }
   ],
   "source": [
    "agent.run(\"can you tell me about the recent cricket worldcup in 250 words?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should use Wikipedia to find information about America's current GDP.\n",
      "Action: wikipedia\n",
      "Action Input: America GDP\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: List of countries by GDP (nominal)\n",
      "Summary: Gross domestic product (GDP) is the market value of all final goods and services from a nation in a given year. Countries are sorted by nominal GDP estimates from financial and statistical institutions, which are calculated at market or government official exchange rates. Nominal GDP does not take into account differences in the cost of living in different countries, and the results can vary greatly from one year to another based on fluctuations in the exchange rates of the country's currency. Such fluctuations may change a country's ranking from one year to the next, even though they often make little or no difference in the standard of living of its population.Comparisons of national wealth are also frequently made based on purchasing power parity (PPP), to adjust for differences in the cost of living in different countries. Other metrics, nominal GDP per capita and a corresponding GDP (PPP) per capita are used for comparing national standard of living. On the whole, PPP per capita figures are less spread than nominal GDP per capita figures.The rankings of national economies over time have changed considerably; the United States surpassed the British Empire's output around 1916, which in turn had surpassed the Qing dynasty in aggregate output decades earlier. Since China's transition to a socialist market economy through controlled privatisation and deregulation, the country has seen its ranking increase from ninth in 1978, to second in 2010; China's economic growth accelerated during this period and its share of global nominal GDP surged from 2% in 1980 to 18% in 2021. Among others, India has also experienced an economic boom since the implementation of economic liberalisation in the early 1990s.The first list includes estimates compiled by the International Monetary Fund's World Economic Outlook, the second list shows the World Bank's data, and the third list includes data compiled by the United Nations Statistics Division. The IMF's definitive data for the past year and estimates for the current year are published twice a year in April and October. Non-sovereign entities (the world, continents, and some dependent territories) and states with limited international recognition (such as Kosovo and Taiwan) are included in the list where they appear in the sources.\n",
      "\n",
      "Page: List of U.S. states and territories by GDP\n",
      "Summary: This is a list of U.S. states and territories by gross domestic product (GDP). This article presents the 50 U.S. states and the District of Columbia and their nominal GDP at current prices.\n",
      "The data source for the list is the Bureau of Economic Analysis (BEA) in 2022. The BEA defined GDP by state as \"the sum of value added from all industries in the state.\"Nominal GDP does not take into account differences in the cost of living in different countries, and the results can vary greatly from one year to another based on fluctuations in the exchange rates of the country's currency. Such fluctuations may change a country's ranking from one year to the next, even though they often make little or no difference in the standard of living of its population.Overall, in the calendar year 2022, the United States' Nominal GDP at Current Prices totaled at $25.463 trillion, as compared to $23.315 trillion in 2021.\n",
      "The three U.S. states with the highest GDPs were California ($3.6 trillion), Texas ($2.356 trillion), and New York ($2.053 trillion). The three U.S. states with the lowest GDPs were Vermont ($40.6 billion), Wyoming ($47.4 billion), and Alaska ($63.6 billion).\n",
      "GDP per capita also varied widely throughout the United States in 2022, with New York ($105,226), Massachusetts ($99,274), and North Dakota ($96,461) recording the three highest GDP per capita figures in the U.S., while Mississippi ($47,572), Arkansas ($54,644), and West Virginia ($54,870) recorded the three lowest GDP per capita figures in the U.S. The District of Columbia, though, recorded a GDP per capita f\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the current GDP of America is $25.463 trillion.\n",
      "Final Answer: The current GDP of America is $25.463 trillion.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The current GDP of America is $25.463 trillion.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"can you tell me the current GDP of America in 1 line?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAI(client=<openai.resources.completions.Completions object at 0x00000184E5645340>, async_client=<openai.resources.completions.AsyncCompletions object at 0x00000184E5649CD0>, openai_api_key='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', openai_proxy='')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"what is a good name for company that makes {product}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm = client, prompt = prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ayush Singh\\anaconda3\\envs\\testingopenai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n1. VinoVeritas\\n2. Grapevine Co.\\n3. Cellar Select\\n4. Vintage Vines\\n5. Red & White Co.\\n6. Cork & Cask\\n7. Bacchus Brews\\n8. WineWise Co.\\n9. Noble Nectar\\n10. Harvest Heights\\n11. Palate Pleasures\\n12. Sip & Savor Co.\\n13. Vineyard Ventures\\n14. Tannin Trail Co.\\n15. The Wine Merchant\\n16. Bottle & Barrel Co.\\n17. Magnum Manor\\n18. Chateau Charm\\n19. Cellar Door Co.\\n20. Grape Expectations'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"Wine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables = ['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food, suggest a fancy name for this\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['cuisine'], template='I want to open a restaurant for {cuisine} food, suggest a fancy name for this')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm = client, prompt = prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\"Spice Palace\"'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"Indian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm = client, prompt = prompt_template, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI want to open a restaurant for American food, suggest a fancy name for this\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\"Stateside Eats\" '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"American\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## if we want to combine multiple chain a set of sequences for that we use simple sequential chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_name = PromptTemplate(\n",
    "input_variables =[\"startup_name\"], \n",
    "template = \"I want to start a startup for {startup_name}, suggest me a good name for this\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_chain = LLMChain(llm = client, prompt = prompt_template_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PromptTemplate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m prompt_template_items \u001b[38;5;241m=\u001b[39m \u001b[43mPromptTemplate\u001b[49m(input_variables \u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      2\u001b[0m                                        template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuggest some straitegies for \u001b[39m\u001b[38;5;132;01m{name}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PromptTemplate' is not defined"
     ]
    }
   ],
   "source": [
    "prompt_template_items = PromptTemplate(input_variables =[\"name\"],\n",
    "                                       template = \"suggest some straitegies for {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "straitegies_chain = LLMChain(llm = client, prompt = prompt_template_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = SimpleSequentialChain(chains = [name_chain, straitegies_chain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Develop a strong online presence: In today's digital age, having a strong online presence is essential for any technology company. MindTechAI should focus on creating a professional website, regularly updating their social media channels, and leveraging digital marketing techniques to reach a wider audience.\n",
      "\n",
      "2. Partner with other tech companies: Collaborating with other technology companies can open up new opportunities for MindTechAI. They can partner with companies that offer complementary products or services to expand their reach and offer a more comprehensive solution to their clients.\n",
      "\n",
      "3. Attend industry events and conferences: Attending industry events and conferences can help MindTechAI network with potential clients and partners. It also allows them to stay updated with the latest trends and technologies in the industry.\n",
      "\n",
      "4. Offer free trials or demos: Offering free trials or demos of their products can be an effective way for MindTechAI to showcase their capabilities and attract potential customers. This also allows them to gather feedback and improve their products based on user experience.\n",
      "\n",
      "5. Focus on customer satisfaction: Satisfied customers can be the best form of marketing for MindTechAI. They should prioritize providing excellent customer service and support to build a loyal customer base and generate positive word-of-mouth.\n",
      "\n",
      "6. Invest in research and development: As a technology company, it\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"artificiall intelligence\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets Understand the Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_name = PromptTemplate(input_variables=['cuisine'],\n",
    "template = \"I want to open a restaurant for {cuisine}, suggest a fancy name for it\")\n",
    "\n",
    "name_chain = LLMChain(llm = client, prompt = prompt_template_name, output_key=\"restaurant_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_items = PromptTemplate(input_variables = ['restaurant_name'],\n",
    "                                       template=\"suggest some menu items for {restaurant_name}\")\n",
    "\n",
    "food_items_chain = LLMChain(llm = client, prompt = prompt_template_items, output_key=\"menu_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = SequentialChain(chains = [name_chain, food_items_chain],\n",
    "                        input_variables =['cuisine'],\n",
    "                        output_variables = ['restaurant_name', 'menu_items' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ayush Singh\\anaconda3\\envs\\testingopenai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cuisine': 'Indian', 'restaurant_name': '\\n\\n\"Royal Spice Bistro\"', 'menu_items': '\\n\\n1. Butter Chicken: A classic Indian dish made with tender chicken in a creamy tomato-based sauce.\\n\\n2. Lamb Rogan Josh: Slow-cooked lamb in a rich and flavorful tomato and onion sauce.\\n\\n3. Vegetable Biryani: Fragrant basmati rice cooked with mixed vegetables, herbs, and spices.\\n\\n4. Tandoori Chicken: Marinated chicken grilled in a traditional clay oven for a smoky and tender flavor.\\n\\n5. Palak Paneer: Creamy spinach and cottage cheese curry, a popular vegetarian dish.\\n\\n6. Chicken Tikka Masala: Grilled chicken in a creamy tomato-based sauce with a blend of spices.\\n\\n7. Samosas: Crispy pastry pockets filled with spiced potatoes and peas, served with a tangy chutney.\\n\\n8. Naan Bread: Traditional Indian flatbread cooked in a tandoor oven.\\n\\n9. Mango Lassi: A refreshing yogurt-based drink with fresh mango puree and spices.\\n\\n10. Gajar Halwa: A decadent Indian dessert made with grated carrots, milk, and cardamom, topped with nuts.'}\n"
     ]
    }
   ],
   "source": [
    "print(chain({\"cuisine\": \"Indian\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(r\"D:\\NLP\\GPT.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.pdf.PyPDFLoader at 0x184e5930ac0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='1\\nGPT (Generative Pre-trained Transformer)  A\\nComprehensive Review on Enabling Technologies,\\nPotential Applications, Emerging Challenges, and\\nFuture Directions\\nGokul Yenduri, Ramalingam M, Chemmalar Selvi G, Supriya Y , Gautam Srivastava,\\nPraveen Kumar Reddy Maddikunta, Deepti Raj G, Rutvij H Jhaveri, Prabadevi B, Weizheng Wang, Athanasios V .\\nVasilakos, and Thippa Reddy Gadekallu\\nAbstract The Generative Pre-trained Transformer (GPT) rep-\\nresent a notable breakthrough in the domain of natural language\\nprocessing, which is propelling us toward the development of\\nmachines that can understand and communicate using language\\nin a manner that closely resembles that of humans. GPT is based\\non the transformer architecture, a deep neural network designed\\nfor natural language processing tasks. Due to their impressive\\nperformance on natural language processing tasks and ability\\nto effectively converse, GPT have gained signicant popularity\\namong researchers and industrial communities, making them one\\nof the most widely used and effective models in natural language\\nprocessing and related elds, which motivated to conduct this\\nreview. This review provides a detailed overview of the GPT,\\nincluding its architecture, working process, training procedures,\\nenabling technologies, and its impact on various applications.\\nIn this review, we also explored the potential challenges and\\nlimitations of a GPT. Furthermore, we discuss potential solutions\\nand future directions. Overall, this paper aims to provide a\\ncomprehensive understanding of GPT, enabling technologies,\\ntheir impact on various applications, emerging challenges, and\\npotential solutions.\\nIndex Terms Generative Pre-trained Transformer, Natural\\nlanguage processing, Articial Intelligence\\nGokul Yenduri, Ramalingam M, Chemmalar Selvi G, Supriya Y , Praveen\\nKumar Reddy Maddikunta, Deepti Raj G, Prabadevi B are with the School\\nof Information Technology and Engineering, Vellore Institute of Technol-\\nogy, Vellore, Tamil Nadu- 632014, India (Emails: {gokul.yenduri, rama-\\nlingam.m, chemmalarselvi.g, supriya.d, praveenkumarreddy, deeptiraj.g2020,\\nprabadevi.b}@vit.ac.in)\\nGautam Srivastava is with the Dept. of Math and Computer Science, Bran-\\ndon University, Canada, and the Research Centre for Interneural Computing,\\nChina Medical University, Taichung, Taiwan as well as Dept. of Computer\\nScience and Math, Lebanese American University, Beirut, Lebanon (email:\\nsrivastavag@brandonu.ca)\\nRutvij H Jhaveri is with the Department of Computer Science and Engi-\\nneering, School of Technology, Pandit Deendayal Energy University, India,\\n(Email: rutvij.jhaveri@sot.pdpu.ac.in).\\nWeizheng Wang is with the Department of Computer Science,\\nCity University of Hong Kong, Hong Kong SAR, China, (E-mail:\\nweizheng.wang@ieee.org).\\nAthanasios V . Vasilakos is with the Center for AI Research\\n(CAIR),University of Agder(UiA), Grimstad, Norway, (Email:\\nthanos.vasilakos@uia.no).\\nThippa Reddy Gadekallu is with the School of Information Technology\\nand Engineering, Vellore Institute of Technology, Vellore 632014, India,\\nLovely Professional University, Phagwara, India, Department of Electrical\\nand Computer Engineering, Lebanese American University, Byblos, Lebanon,\\nJiaxing University , Jiaxing 314001, China, Zhongda Group, China, 314312\\n(E-mail: thippareddy@ieee.org).TABLE I\\nLIST OF KEY ACRONYMS ONLY IF IT IS REPEATED\\nAcronyms Description\\nAI Articial Intelligence\\nAR Augmented Reality\\nBERT Bidirectional Encoder Representations from\\nTransformers\\nBGN BonehGohNissim\\nCNN ConvolutionalNeural Network\\nDAP Data Access Point\\nDLT Decentralized Ledger Technology\\nDL Deep Learning\\nDRL Deep Reinforcement Learning\\nDR Demand response\\nEC Edge Computing\\nEU End User\\nEAPs Energy Access Points\\n5G Fifth-Generation\\n4G Fourth-Generation\\nGPT Generative Pre-trained Transformer\\nGPU Graphics Processing Unit\\nHPC High Performance Computing\\nHCI Human Computer Interaction\\nIoT Internet of Things\\nML Machine Learning\\nNLP Natural Language Processing' metadata={'source': 'D:\\\\NLP\\\\GPT.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(pages[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_key = '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'\n",
    "client = OpenAI(openai_api_key = my_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_name = PromptTemplate(input_variables=['product'], \n",
    "                                      template=\"what is a good name for a company taht makes a {product}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=client,prompt = prompt_template_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. ChromaCups\n",
      "2. Rainbow Rims\n",
      "3. Prismware\n",
      "4. Vibrant Vessels\n",
      "5. Spectrum Sippers\n",
      "6. ColorSplash Cups\n",
      "7. KaleidoKups\n",
      "8. Tinted Tumblers\n",
      "9. Chromatic Chalices\n",
      "10. Cloudburst Containers\n"
     ]
    }
   ],
   "source": [
    "print(chain.run('colorful cups').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"SkyTech Aerial Solutions\"\n"
     ]
    }
   ],
   "source": [
    "print(chain.run('Drones').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chain.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ConversationBufferMemory\n",
    "#We can attach memory to remember all previous conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_name = PromptTemplate(input_variables=['product'],\n",
    "                                      template = \"what would be a good name for a company that makes a {product}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm = client, prompt = prompt_template_name, memory = memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Vintage Vines Co.\"\n"
     ]
    }
   ],
   "source": [
    "print(chain.run('wines').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Capture Co.\"\n"
     ]
    }
   ],
   "source": [
    "print(chain.run('camera').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. BatCrafters\n",
      "2. Winged Wonders\n",
      "3. BatWorks\n",
      "4. Nightflyers\n",
      "5. Bat Brigade\n",
      "6. FlitterFits\n",
      "7. Bat Builders\n",
      "8. Winged Creations\n",
      "9. Bat Blaze\n",
      "10. Nocturnal Niche\n",
      "11. BatBrews\n",
      "12. Cave Creators\n",
      "13. The Bat Company\n",
      "14. Aerobatix\n",
      "15. Batty Builders\n"
     ]
    }
   ],
   "source": [
    "print(chain.run('bats').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='wines'), AIMessage(content='\\n\\n\"Vintage Vines Co.\" '), HumanMessage(content='camera'), AIMessage(content='\\n\\n\"Capture Co.\" '), HumanMessage(content='bats'), AIMessage(content='\\n\\n1. BatCrafters\\n2. Winged Wonders\\n3. BatWorks\\n4. Nightflyers\\n5. Bat Brigade\\n6. FlitterFits\\n7. Bat Builders\\n8. Winged Creations\\n9. Bat Blaze\\n10. Nocturnal Niche\\n11. BatBrews\\n12. Cave Creators\\n13. The Bat Company\\n14. Aerobatix\\n15. Batty Builders')]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: wines\n",
      "AI: \n",
      "\n",
      "\"Vintage Vines Co.\" \n",
      "Human: camera\n",
      "AI: \n",
      "\n",
      "\"Capture Co.\" \n",
      "Human: bats\n",
      "AI: \n",
      "\n",
      "1. BatCrafters\n",
      "2. Winged Wonders\n",
      "3. BatWorks\n",
      "4. Nightflyers\n",
      "5. Bat Brigade\n",
      "6. FlitterFits\n",
      "7. Bat Builders\n",
      "8. Winged Creations\n",
      "9. Bat Blaze\n",
      "10. Nocturnal Niche\n",
      "11. BatBrews\n",
      "12. Cave Creators\n",
      "13. The Bat Company\n",
      "14. Aerobatix\n",
      "15. Batty Builders\n"
     ]
    }
   ],
   "source": [
    "print(chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ConversationChain**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conversation buffer memory goes growing endlessly\n",
    "\n",
    "Just remember last 5 Conversation chain and if just remember 10-20 conversations chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo = ConversationChain(llm = OpenAI(openai_api_key = my_key, temperature=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['history', 'input'], template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "print(convo.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The first cricket worldcup was held in 1975 and was won by the West Indies team. The captain of the winning team was Clive Lloyd.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who won the first cricket worldcup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Yes, of course! 5+5 is equal to 10. This is a simple addition problem and the answer is derived from the fact that 5+5 is the same as adding 5 to itself, which results in 10. This is a basic math concept that is usually learned in early education.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"can you tell me what is 5+5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Sure! 5+(7*3-2) is equal to 24. Let me break it down for you. First, we have 7 multiplied by 3, which equals 21. Then, we subtract 2 from that, giving us 19. Finally, we add 5 to that, resulting in 24. Does that make sense?'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"can you tell me what is 5+(7*3-2)?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I'm sorry, I do not have enough context to answer that question. Can you provide more information?\""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"can you tell me who was the captain of the winning team?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo = ConversationChain(llm = OpenAI(openai_api_key = my_key, temperature = 0.1),memory = memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The first Cricket World Cup was held in 1975 and was won by the West Indies team. They defeated Australia in the final match by 17 runs. The tournament was hosted by England and was played in a round-robin format with eight teams participating. The West Indies team was led by Clive Lloyd and had players like Viv Richards, Gordon Greenidge, and Michael Holding. It was a historic moment for the West Indies as it was their first World Cup victory.'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"who won the first cricket worldcup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Of course! 5+5 is equal to 10. This is a basic addition problem and the answer is a simple sum of the two numbers. Did you know that the concept of addition has been around since ancient times? The ancient Egyptians and Babylonians used different methods to add numbers, such as using tally marks or a counting board. It wasn't until the 16th century that the modern addition symbol (+) was introduced by German mathematician Johannes Widmann.\""
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"can you tell me how much will be 5+5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The captain of the winning team, the West Indies, was Clive Lloyd.'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"who was the captain of winning team, answer in only 1 line?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testingopenai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
